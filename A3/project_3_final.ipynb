{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWQZNpJgLAZC"
      },
      "source": [
        "# Project 3. Limited time training\n",
        "\n",
        " In project 3, you're going to improve your network under the limitation of time, specifically 10 minutes. You can borrow any structure that you consider viable, but pretrained models are **NOT allowed**.\n",
        "\n",
        "**Hint**: Most of the popular networks can't converge in 10 minutes, so if you decide to borrow any, you'd have to trim the network a little to fit into the time frame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vBi1AzTSoES"
      },
      "source": [
        "## 1. Preparation\n",
        "\n",
        "This project is time limited, we need to make sure that your hardwares are exactly the same, try the following code, if you see 'GPU correct, proceed.', you can continue the project, otherwise, click the dropdown at the upper right corner besides the status bar (RAM, Disk), and go to 'Manage sessions', and terminate the current session, then reconnect, and try the code again, repeat until you see the right message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1uoQawsBsdD",
        "outputId": "4d662b0c-30a7-4d91-f077-e821be3fba67"
      },
      "outputs": [],
      "source": [
        "!if [ $(nvidia-smi | grep Tesla | tr -s \" \" | cut -d' ' -f4) == 'T4' ]; then echo \"GPU correct, proceed.\"; else echo \"GPU incorrect, please terminate this session and reconnect again.\"; fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFg9DbPZU5s8"
      },
      "source": [
        "## 2. Before training\n",
        "\n",
        "For dataset, we continue to use CIFAR10, in this part, you can configure all of your settings, including data augmentation, model structure, optimizer etc., but don't put any major calculation part in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk--hkA1agMz"
      },
      "source": [
        "### 2.1 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "d4198afcb45c4666903c3bac01ac13e9",
            "72a15f76c66a4ab2976f2f594d13efb3",
            "883f59c93950402581275bf7b4fa879d",
            "8c2382300eca4652b4fcacecd932229b",
            "75c8f23dbca54b9d8bba7210d94c84bf",
            "77b6159983954a00a48f86829e65b06c",
            "9bb2288b810144b3b0de9124b8086af7",
            "033d6d4e11b442e0b93b4bb986f99f52"
          ]
        },
        "id": "GcyQM7qiLagn",
        "outputId": "884554b6-13a3-44c9-f487-b0c3a9a2adae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torchvision.transforms as tt\n",
        "\n",
        "stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "train_tfms = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'), \n",
        "                         tt.RandomHorizontalFlip(), \n",
        "                         tt.ToTensor(), \n",
        "                         tt.Normalize(*stats,inplace=True)])\n",
        "valid_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])\n",
        "\n",
        "# cifar10_train = torchvision.datasets.CIFAR10(\"CIFAR10\", train=True, transform=lambda x: np.array(x, dtype=np.float32).transpose(2,0,1),\n",
        "#                                          target_transform=None, download=True)\n",
        "# cifar10_test = torchvision.datasets.CIFAR10(\"CIFAR10\", train=False, transform=lambda x: np.array(x, dtype=np.float32).transpose(2,0,1),\n",
        "#                                          target_transform=None, download=True)\n",
        "\n",
        "cifar10_train = torchvision.datasets.CIFAR10(\"CIFAR10\", train=True, transform=train_tfms,\n",
        "                                         target_transform=None, download=True)\n",
        "cifar10_test = torchvision.datasets.CIFAR10(\"CIFAR10\", train=False, transform=valid_tfms,\n",
        "                                         target_transform=None, download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vcCJYrGax4N"
      },
      "source": [
        "### 2.2 Model\n",
        "\n",
        "**Hint**: Even though pretrained models are not allowed in your final version, you can use pretrained models to determine how well the structure CAN perform and use this as part of your analysis as to determining network structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pUWX3Ip3p9e"
      },
      "source": [
        "**Model definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "brj3qQkNLnlO"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m         out \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.125\u001b[39m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m---> 47\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mResNet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:911\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:911\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:302\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    301\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    306\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination"
          ]
        }
      ],
      "source": [
        "# resnet-9 5GB - 88.5\n",
        "# resnet-18 10GB - not finished\n",
        "# my net 3.5GB 86\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "def conv_block(in_channels, out_channels, pool=False, eps=0.00001, mon=0.1, dropout=0):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3,stride=1, padding=1, bias=False,padding_mode='zeros'),\n",
        "              nn.BatchNorm2d(out_channels,eps,mon),\n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    # if dropout > 0: layers.append(nn.Dropout(dropout))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.conv1 = conv_block(in_channels, 64, eps=1e-5, mon=0.9)\n",
        "        self.conv2 = conv_block(64, 128, eps=1e-5, mon=0.9)\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128, eps=1e-5, mon=0.9), conv_block(128, 128, eps=1e-5, mon=0.9))\n",
        "\n",
        "        self.conv3 = conv_block(128, 256, pool=True, eps=1e-5, mon=0.9)\n",
        "        self.conv4 = conv_block(256, 512, pool=True, eps=1e-5, mon=0.9)\n",
        "        self.res2 = nn.Sequential(conv_block(512, 512, eps=1e-5, mon=0.9), conv_block(512, 512, eps=1e-5, mon=0.9))\n",
        "\n",
        "        self.classifier = nn.Sequential(nn.AdaptiveMaxPool2d((1,1)),\n",
        "                                        nn.Flatten(),\n",
        "                                        # nn.Dropout(0.1),\n",
        "                                        nn.Linear(512, num_classes, bias=False))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.classifier(out)\n",
        "        out *= 0.125\n",
        "        return out\n",
        "\n",
        "net = ResNet(3, 10).cuda()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHUS2sSd3toa"
      },
      "source": [
        "**Analysis tools**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb_Ume_bHEr0",
        "outputId": "54a4ceb1-b168-4bc4-ffdb-2dc558c0e2bb"
      },
      "outputs": [],
      "source": [
        "# @torch.compile  \n",
        "def testset_precision(net, testset):\n",
        "    net.eval()\n",
        "    dl = DataLoader(testset, batch_size=512)\n",
        "    total_count = 0\n",
        "    total_correct = 0\n",
        "    for data in dl:\n",
        "        inputs = data[0].cuda()\n",
        "        targets = data[1].cuda()\n",
        "        outputs = net(inputs)\n",
        "        predicted_labels = outputs.argmax(dim=1)\n",
        "        comparison = predicted_labels == targets\n",
        "        total_count += predicted_labels.size(0)\n",
        "        total_correct += comparison.sum()\n",
        "    net.train()\n",
        "        \n",
        "    return int(total_correct) / int(total_count)\n",
        "\n",
        "print(f'Inital precision: {testset_precision(net, cifar10_test)}')\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "class DrawingBoard:\n",
        "    def __init__(self, names, time_slot=60):\n",
        "        self.start_time = time.time()\n",
        "        self.time_slot = time_slot\n",
        "        self.annotations = []\n",
        "        self.data = {}\n",
        "        for name in names:\n",
        "            self.data[name] = []\n",
        "    \n",
        "    def update(self, data_dict):\n",
        "        for key in data_dict:\n",
        "            self.data[key].append(data_dict[key])\n",
        "        current_time = time.time() - self.start_time\n",
        "        idx = len(self.data[key]) -1\n",
        "        if len(self.annotations) == 0:\n",
        "            if current_time > self.time_slot:\n",
        "                self.annotations.append((idx, current_time))\n",
        "        elif current_time - (self.annotations[-1][1]//self.time_slot)*self.time_slot > self.time_slot:\n",
        "            self.annotations.append((idx, current_time))\n",
        "    \n",
        "    def draw(self):\n",
        "        all_keys = list(self.data.keys())\n",
        "        fig, ax = plt.subplots(nrows=1, ncols=len(all_keys))\n",
        "        fig.set_figwidth(20)\n",
        "        for idx in range(len(all_keys)):\n",
        "            ax[idx].plot(self.data[all_keys[idx]])\n",
        "            ax[idx].grid()\n",
        "            ax[idx].set_title(all_keys[idx])\n",
        "            for an in self.annotations:\n",
        "                ax[idx].annotate(f'{int(an[1])}s', xy=(an[0], self.data[all_keys[idx]][an[0]]),\n",
        "                    xytext=(0, -40), textcoords=\"offset points\",\n",
        "                    va=\"center\", ha=\"left\",\n",
        "                    bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
        "                    arrowprops=dict(arrowstyle=\"->\"))\n",
        "        display.clear_output(wait=True)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgkJbvpOOa6m"
      },
      "outputs": [],
      "source": [
        "# modify as you need\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5) # 3, 0.85\n",
        "dataloader = DataLoader(cifar10_train, batch_size=512, shuffle=True, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwkS_qPD3y6E"
      },
      "source": [
        "## 3. Start Training\n",
        "\n",
        "This is the training part, take advantage of everything you know. In addition to the tips last time, you can also consider data augmentation, short cut structure in ResNet etc.\n",
        "\n",
        "Basically at this point, nothing is limited except time and pretrained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "ox4DiCIyP835",
        "outputId": "13565f5e-f2f1-4e25-a9ca-c1516cb9bbff"
      },
      "outputs": [],
      "source": [
        "# modify as you need\n",
        "import time\n",
        "import signal\n",
        "class TimeLimitation:\n",
        "    def __init__(self, limit):\n",
        "        self.limit = limit\n",
        "\n",
        "    def __enter__(self):\n",
        "        def handler(signum, frame):\n",
        "            raise NotImplementedError('Time\\'s up')\n",
        "        signal.signal(signal.SIGALRM, handler)\n",
        "        signal.alarm(self.limit)\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        signal.alarm(0)\n",
        "\n",
        "\n",
        "with TimeLimitation(60*60*2): # don't forget the indentation\n",
        "    print('start training')\n",
        "    db = DrawingBoard(['training_loss', 'testset_precision'], time_slot=60)\n",
        "    for epoch in range(500):\n",
        "        for iter, data in enumerate(dataloader):\n",
        "            inputs = data[0].cuda()\n",
        "            targets = data[1].cuda()\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
        "            optimizer.step()\n",
        "\n",
        "            if iter % 20 == 0:\n",
        "                num = testset_precision(net, cifar10_test)\n",
        "                print(num)\n",
        "                db.update({'training_loss':loss.item(), 'testset_precision': num})\n",
        "                db.draw()\n",
        "        scheduler.step()\n",
        "\n",
        "    print('finished')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WST0cHoKPZ02"
      },
      "source": [
        "# Marking Scheme\n",
        "\n",
        "*   Code implementation: 30%\n",
        "\n",
        "\n",
        "> * Successfully run through the whole project: 10%\n",
        "> * Code implementation quality: 20% (including performance)\n",
        "\n",
        "*   PDF report: 50%\n",
        "\n",
        "> * Overview, introduction to the network structure etc.: 10%\n",
        "> * In-depth analysis and improvements: 40%\n",
        "\n",
        "*   Presentation: 20%\n",
        "\n",
        "> * Overall demonstration: 10%\n",
        "> * Unfolding analysis: 10%"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "033d6d4e11b442e0b93b4bb986f99f52": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72a15f76c66a4ab2976f2f594d13efb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75c8f23dbca54b9d8bba7210d94c84bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "77b6159983954a00a48f86829e65b06c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "883f59c93950402581275bf7b4fa879d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77b6159983954a00a48f86829e65b06c",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75c8f23dbca54b9d8bba7210d94c84bf",
            "value": 170498071
          }
        },
        "8c2382300eca4652b4fcacecd932229b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_033d6d4e11b442e0b93b4bb986f99f52",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9bb2288b810144b3b0de9124b8086af7",
            "value": " 170499072/? [00:20&lt;00:00, 8304427.15it/s]"
          }
        },
        "9bb2288b810144b3b0de9124b8086af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4198afcb45c4666903c3bac01ac13e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_883f59c93950402581275bf7b4fa879d",
              "IPY_MODEL_8c2382300eca4652b4fcacecd932229b"
            ],
            "layout": "IPY_MODEL_72a15f76c66a4ab2976f2f594d13efb3"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
